---
layout:     post
title:      "Deep Reinforcement Learning-DQN paper&code cookbook"
subtitle:   "DQN 论文解读以及基于Baselines的DQN代码解析"
date:       2018-06-05
author:     "Joey"
catalog:      true
tags:
    - Reinforcement Learning
---



# DQN

## DQN思想解读

> 参考文献
>
> [[1] Playing Atari with Deep Reinforcement Learning](https://pdfs.semanticscholar.org/667f/b84bfca10bec165f9e2cca3e21f5e4829ca7.pdf) 
> [[2] Human-level control through deep reinforcement learning](http://www.nature.com/nature/journal/v518/n7540/abs/nature14236.html)

DQN是基于`Q-Learning`的升级, 是Q-Learning和神经网络结合的产物.

### 神经网络

**提出神经网络的原因**

学习Q-Learning时, 我们使用的都是通过表格形式来存储每一个状态 state, 和在这个 state 每个行为 action 所拥有的 Q 值. 而当今问题实在太复杂, state多如牛毛, 如果仍然用全表格来存储, 再大的内存也不够用, 况且搜索也是一件耗时的事情.

基于这个背景, 提出了神经网络的方法: 

- 我们可以将状态和动作当成神经网络的输入, 然后经过神经网络分析后得到动作的 Q 值, 这样我们就没必要在表格中记录 Q 值, 而是直接使用神经网络生成 Q 值. 

- 还有一种形式的是这样, 我们也能只输入状态值, 输出所有的动作值, 然后按照 Q learning 的原则, 直接选择拥有最大值的动作当做下一步要做的动作

  ![NN](/img/in-post/rl/NN.jpg)



**更新神经网络**

前面说到我们可以 输入state, 通过神经网络, 来预测所有的action以及对应的value, 但是神经网络先要被训练好才能预测出准确的值, 那么如何训练?

还记得Q-Learning的公式吗? 

$Q(S_t, A_t) = Q(S_t, A_t) + \alpha [R_{t+1} + \gamma max_{a'} Q(S_{t+1}, A') - Q(S_t, A_t) ]$

通俗来说, 就是   $新的Q = 旧的Q + \alpha(target Q - behavior Q) $

神经网络的本质就是 `用一个非线性函数逼近器来表征一个 action-value function即Q(s,a)` . 所以训练神经网络就是使其输出准确的 Q(s,a). 所以神经网络的更新就是:

$新的NN = 旧的NN + \alpha (target Q - behavior Q)'$

流程整理一下:

1. 假设此刻我们位于状态s1
2. 我们通过采取行动估算出 Q(s1,a1) 和 Q(s1, a2)的值. 可以认为是behavior policy的Q
3. 然后我们通过 $\epsilon-greedy$ 策略 (以概率 $\epsilon$ 选取随机action, 否则选择能使Q最大的action) 选取action, 假设这里选择的是a1
4. 执行action来获得到达下一个状态s2 的即时奖励reward R
5. 此时根据 target policy , 预测Q(s2,a1), Q(s2, a2), 选取能使Q最大的action, 假设为a2 , 注意: 这里并没有采取行动, 只是根据采样数据估算出来的Q(s2). 此刻的 $Q(s1, a1) = 到达s2时的即时奖励 R + \gamma max_{a'}Q(s2) $ . 这个Q(s1, a1) 是target Policy所得到的Q, 可以认为是训练神经网络用的Label . 
6. target Q(s1, a1) 和 behavior Q(s1, a1) 之间的差异, 也可以认为是label与预测值之间的差异,  乘以一个衰减值 $\alpha$ , 即 $ TD Error = \alpha [R_{t+1} + \gamma max_{a'} Q(S_{t+1}, A') - Q(S_t, A_t) ]$ ,  就可以通过计算td_error 的 huber_loss来 更新神经网络的参数.

### 技巧

有了神经网络更新方法, 我们仍然不能即刻着手训练. 这是由于训练神经网络时要求训练数据满足独立同分布的要求，而由强化学习算法采样回来的数据是不满足这个要求的：

1. 不满足独立的要求，因为强化学习处理的是序列问题，数据间的相关性很大。
2. 不满足同分布要求，因为状态分布会随着策略的更新而改变。

所以DQN提出了2个创新方法来解决这个问题：

1. 经验回放(experience replay)，主要用于解决样本数据不独立的问题。该方法通过存储过去的经验，并随机打乱之后再用于神经网络的训练，来打破数据样本之间的相关性。
2. Fixed Q-targets, 延迟更新 target policy，用于解决样本数据不满足同分布的问题。由于状态分布会随着策略的更新而改变，这会导致策略的收敛过程发生严重的震荡，减缓收敛速度，甚至导致策略不收敛。那么，通过延迟更新 target policy，我们可以保证在一段时间内， target policy 的状态分布不变。可以认为，在这段时间内，算法的收敛是比较稳定的。

#### Experience replay

这个机制的原理很简单。在算法运行过程中采集到的样本会被不断地储存到一个大小固定的回放记忆体中，在需要训练神经网络时再从这个记忆体中随机采样出若干个样本使用即可。这个机制的优点总结如下，

1. 数据会被重复利用，提高其利用率
2. 破坏样本之间的相关性，从而降低参数更新的方差
3. 可以避免神经网络总是被偏向于某个状态的样本训练

在实际使用时，在算法刚开始运行的时候，记忆体中的样本需要积攒到一定数量时才会开始训练神经网络。当记忆体空间被用光之后，其中的样本会以一种先进先出的方式被删除。这其实不是一种很好的删除方式，因为这样可能会删除重要的样本。更好的方法是使用类似 prioritized sweeping 这样的方法来有选择性地删除不重要的样本。

#### Fixed Q-targets

在上面所示的参数更新方法中，target policy 的Q网络是在每个训练循环结束后被立即更新的，因为 behaviour policy 和 target policy 的Q网络用的是同一套网络参数。这样就会导致每次网络更新之后 target policy 生成的状态分布发生变化，使得训练变得不稳定。
为了减轻这个问题带来的影响，我们需要准备2套网络参数。其中一套参数$\theta^-$用于 target policy 的Q网络，而另一套参数$\theta$用于 behaviour policy 的Q网络。在训练时算法直接更新 behaviour policy 的Q网络，而不更新 target policy 的Q网络。这是为了保证在一段时间内， target policy 生成的状态分布不变，从而可以保证一定的训练稳定性。在一段时间之后，算法会将两个网络进行同步，以便 behaviour policy 可以逐步向最优解靠拢。这里同步网络的周期是一个人为设定的超参数。
此时，神经网络的损失函数及其导数可以写作，
$L(\theta)=\mathbb{E}[(r+\gamma\max_{a'}Q(s',a';\theta^-)-Q(s,a;\theta))^2]$
$\nabla_\theta L(\theta)=\mathbb{E}[(r+\gamma\max_{a'}Q(s',a';\theta^-)-Q(s,a;\theta))\nabla_\theta Q(s,a;\theta)]$

在 `Q-Learning` 中, 

$TDError= r+\gamma\max_{a'}Q(s',a';\theta)-Q(s,a;\theta)$

**注意:** 如果Q使用函数来表征, 而非table, 那么函数的参数 $\theta​$ 是唯一决定Q的因素. state和action只是确定在某个具体状态下采取某个行为的具体value.

所以可以看到, target Q 和behavior Q是同一个 Q, 他们共享同一套参数, 只是target Q 在状态S下根据greedy 策略选择能使下一个状态 S' 的Q value最大的一个action, 而behavior Q 根据 $\epsilon - greedy$ 选择action, 到达状态S.

而在 `DQN` 中, 使用fixed Q-targets方法后, target Q的参数和behavior Q的参数是不同的. 在DQN中的 td_error为:

$TDError= r+\gamma\max_{a'}Q(s',a';\theta^-)-Q(s,a;\theta)$

通过延迟更新target 网络参数 $\theta^-$  , 保证 target policy生成的Q 分布较稳定, 从而保证一定的训练稳定性。因为target Policy给出的 $ Q:r+\gamma\max_{a'}Q(s',a';\theta^-)$  , 可认为是神经网络的Label. 保证label的稳定, 可便于最终能够逐步向最优解靠拢.



### 伪代码

![dqn_algorithm](/img/in-post/rl/dqn_algorithm.png)



伪代码中文版:

- 初始化回放记忆D，可容纳的数据条数为N

- 利用随机权值 $\theta$ 来初始化动作-行为值函数Q

- 令 $\theta^-$  = $\theta$ 初始化用来计算TD目标的动作行为值Q

- 循环每次episode:

  - 初始化事件的第一个状态s1 ,预处理得到状态对应的特征输入

  - 循环每个episode的每一个step:

    - 利用概率 $\epsilon$ 选一个随机动作 $a_t$  

    - 如果小概率事件没发生，则用贪婪策略选择当前值函数最大的那个动作 $a_t = \mathrm{argmax}_aQ(s,a;\theta)$  .  注意，这里选最大动作时用到的值函数网络与逼近值函数所用的网络是一个网络，都对应着$\theta$ .

      注意: 以上两步为 行动策略, 即 $\epsilon-greedy$ 策略.

    - 在Env中(仿真中)执行动作 $a_t$  , 观测回报 $r_t$ 以及图像 $x_{t+1}$.

    - 设置 $s_{t+1}=s_t, a_t, x_{t+1}$, 

    - 将转换 $<s_t, a_t, r_t, s_{t+1}>$ 存储在回放记忆D中

    - 从回放记忆D中**均匀随机采样**一个转换样本数据，用$<s_j, a_j, r_j, s_{j+1}>$ 来表示

    - 判断是否是一个事件的终止状态，若是终止状态则TD目标为 $r_j$ , 否则利用TD目标网络$\theta^-$ 计算TD目标 $r+\gamma\max_{a'}Q(s',a';\theta^-)$ .

    - 执行一次梯度下降算法 $\Delta\theta=[r+\gamma\max_{a'}Q(s',a';\theta^-)-Q(s,a;\theta)]\nabla Q(s,a;\theta)$.

    - 更新动作值函数逼近的网络参数  $\theta = \theta + \Delta\theta$ .

    - 每隔C步更新一次TD目标网络权值即令 $\theta^-$  = $\theta$ .

- Over

## DQN代码分析(基于baseline)

**文件结构**

deepq的代码结构如下图所示：

![deepq_structure](/img/in-post/rl/deepq_structure.png)

### Train流程

Train流程脚本实现, 位于 `deepq/experiments/run_atari.py` 文件下:

```python
def main():
	## 脚本运行参数解析
    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument('--env', help='environment ID', default='BreakoutNoFrameskip-v4')
    parser.add_argument('--seed', help='RNG seed', type=int, default=0)
    parser.add_argument('--prioritized', type=int, default=1)
    parser.add_argument('--dueling', type=int, default=1)
    parser.add_argument('--num-timesteps', type=int, default=int(10e6))
    args = parser.parse_args()
	
	## 配置日志
	# 注意在此处指定日志存储路径会比较好，否则是存在临时文件夹/tmp里的
    logger.configure()
	
    set_global_seeds(args.seed)
	
	## 配置环境
    env = make_atari(args.env)
    env = bench.Monitor(env, logger.get_dir())
    env = deepq.wrap_atari_dqn(env)
	
	## 配置模型结构
    model = deepq.models.cnn_to_mlp(
        convs=[(32, 8, 4), (64, 4, 2), (64, 3, 1)],
        hiddens=[256],
        dueling=bool(args.dueling),
    )
	
	## 运行训练脚本
    act = deepq.learn(
        env,
        q_func=model,
        lr=1e-4,
        max_timesteps=args.num_timesteps,
        buffer_size=10000,
        exploration_fraction=0.1,
        exploration_final_eps=0.01,
        train_freq=4,
        learning_starts=10000,
        target_network_update_freq=1000,
        gamma=0.99,
        prioritized_replay=bool(args.prioritized)
    )
	
	# 模型的snapshot默认保存至/tmp临时文件夹下
	# 如需将最终模型保存至指定目录，那么建议打开下面这个注释
    # act.save("pong_model.pkl") XXX
    env.close()
```



#### 1. 配置 Logger

#### 2. 配置 Env

对Atari游戏进行预处理, 代码位于 `openai-baselines/common/atari_wrappers.py` 文件夹下, 我们先来看一下他们一共为Atari准备了多少Wrapper。

**wrapper说明 : **

>Wrapper类是Env类的子类。由此可见，由Wrapper所实例化的对象其本质就是个Env对象。
>那么这么做有什么用呢？我们可以看到，由于Wrapper继承了Env，所以它和Env类必须要有很多一样的方法，而同时，我们需要用一个Env对象来初始化Wrapper。这就意味着，我们可以自定义一个MyWrapper，用它来包裹另一个Env对象，并对这个对象的step, reset, close等方法添加额外的功能。而对于外部调用函数来说，这个Wrapper是透明的，它看见的仍然是一个API完全相同的Env对象。这么做的好处就是可以在完全不影响外部调用函数的情况下，对Env对象添加任意多个功能！
>另外，我们可以按照任务所需的逻辑顺序在Wrapper外面继续套Wrapper来添加一层又一层的额外功能。这么做的好处是可以设计成一个Wrapper只负责添加一个功能，保证代码的低耦合和可重复使用性。
>当我们在一个Env对象外面套了好几层Wrapper之后，该如何获得包裹在最内部的Env对象呢？此时需要用到Env类里的unwrapped方法。这个方法利用递归将最内部的Env对象给返回出来。



##### NoopResetEnv

在重置游戏时，随机执行若干步NOOP动作。NOOP的意思是不做任何事情。这个wrapper的目的是随机化初始状态。这个wrapper对环境的reset方法添加了额外功能。

```python
class NoopResetEnv(gym.Wrapper):
    def __init__(self, env, noop_max=30):
        """Sample initial states by taking random number of no-ops on reset.
        No-op is assumed to be action 0.
        """
        gym.Wrapper.__init__(self, env)
        self.noop_max = noop_max
        self.override_num_noops = None
        self.noop_action = 0
        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'

    def reset(self, **kwargs):
        """ Do no-op action for a number of steps in [1, noop_max]."""
        self.env.reset(**kwargs)
        if self.override_num_noops is not None:
            noops = self.override_num_noops
        else:
            noops = self.unwrapped.np_random.randint(1, self.noop_max + 1) #pylint: disable=E1101
        assert noops > 0
        obs = None
        for _ in range(noops):
            obs, _, done, _ = self.env.step(self.noop_action)
            if done:
                obs = self.env.reset(**kwargs)
        return obs

    def step(self, ac):
        return self.env.step(ac)
```

##### MaxAndSkipEnv

每4帧画面，返回一次obs和reward。在这4帧画面期间，每一帧都重复上一帧的action，最后将4帧的reward加起来作为最后返回的reward。同时，对最后2帧画面取max操作，将合并后的obs作为最后返回的obs。这个wrapper对环境的step方法添加了额外功能。

```python
class MaxAndSkipEnv(gym.Wrapper):
    def __init__(self, env, skip=4):
        """Return only every `skip`-th frame"""
        gym.Wrapper.__init__(self, env)
        # most recent raw observations (for max pooling across time steps)
        self._obs_buffer = np.zeros((2,)+env.observation_space.shape, dtype=np.uint8)
        self._skip       = skip

    def reset(self):
        return self.env.reset()

    def step(self, action):
        """Repeat action, sum reward, and max over last observations."""
        total_reward = 0.0
        done = None
        for i in range(self._skip):
            obs, reward, done, info = self.env.step(action)
            if i == self._skip - 2: self._obs_buffer[0] = obs
            if i == self._skip - 1: self._obs_buffer[1] = obs
            total_reward += reward
            if done:
                break
        # Note that the observation on the done=True frame
        # doesn't matter
        max_frame = self._obs_buffer.max(axis=0)

        return max_frame, total_reward, done, info
```

##### EpisodicLifeEnv

对于带有生命计数的游戏，修改其done属性，将其重定义为每条命结束即代表一个episode结束。这个wrapper对环境的reset方法和step方法添加了额外功能。

```python
class EpisodicLifeEnv(gym.Wrapper):
    def __init__(self, env):
        """Make end-of-life == end-of-episode, but only reset on true game over.
        Done by DeepMind for the DQN and co. since it helps value estimation.
        """
        gym.Wrapper.__init__(self, env)
        self.lives = 0
        self.was_real_done  = True

    def step(self, action):
        obs, reward, done, info = self.env.step(action)
        self.was_real_done = done
        # check current lives, make loss of life terminal,
        # then update lives to handle bonus lives
        lives = self.env.unwrapped.ale.lives()
        if lives < self.lives and lives > 0:
            # for Qbert sometimes we stay in lives == 0 condtion for a few frames
            # so its important to keep lives > 0, so that we only reset once
            # the environment advertises done.
            done = True
        self.lives = lives
        return obs, reward, done, info

    def reset(self, **kwargs):
        """Reset only when lives are exhausted.
        This way all states are still reachable even though lives are episodic,
        and the learner need not know about any of this behind-the-scenes.
        """
        if self.was_real_done:
            obs = self.env.reset(**kwargs)
        else:
            # no-op step to advance from terminal/lost life state
            obs, _, _, _ = self.env.step(0)
        self.lives = self.env.unwrapped.ale.lives()
        return obs
```

##### FireResetEnv

对于那些需要‘开火’之后才会正式开始运行的游戏，先‘开火’一下。这个wrapper对环境的reset方法添加了额外功能。

```python
class FireResetEnv(gym.Wrapper):
    def __init__(self, env):
        """Take action on reset for environments that are fixed until firing."""
        gym.Wrapper.__init__(self, env)
        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'
        assert len(env.unwrapped.get_action_meanings()) >= 3

    def reset(self, **kwargs):
        self.env.reset(**kwargs)
        obs, _, done, _ = self.env.step(1)
        if done:
            self.env.reset(**kwargs)
        obs, _, done, _ = self.env.step(2)
        if done:
            self.env.reset(**kwargs)
        return obs

    def step(self, ac):
        return self.env.step(ac)
```

##### WarpFrame

将图片转成灰度图，并且将图片大小压缩至84\*84。

和论文中不同的是，这里**将图片转换为了灰度图，而不是提取了Y通道**，目前不清楚这2个操作是不是等价的。

ObservationWrapper是Wrapper的延伸，它添加了一个observation方法，我们可以在其中自定义我们希望对step所返回的obs所进行的处理。

```python
class WarpFrame(gym.ObservationWrapper):
    def __init__(self, env):
        """Warp frames to 84x84 as done in the Nature paper and later work."""
        gym.ObservationWrapper.__init__(self, env)
        self.width = 84
        self.height = 84
        self.observation_space = spaces.Box(low=0, high=255,
            shape=(self.height, self.width, 1), dtype=np.uint8)

    def observation(self, frame):
        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)
        frame = cv2.resize(frame, (self.width, self.height), interpolation=cv2.INTER_AREA)
        return frame[:, :, None]
```

##### ScaledFloatFrame

将图像数值归一化为0-1之间的浮点数。论文中没有提到这个操作。可能提取Y通道=灰度+归一化？

```python
class ScaledFloatFrame(gym.ObservationWrapper):
    def __init__(self, env):
        gym.ObservationWrapper.__init__(self, env)

    def observation(self, observation):
        # careful! This undoes the memory optimization, use
        # with smaller replay buffers only.
        return np.array(observation).astype(np.float32) / 255.0
```

##### ClipRewardEnv

将reward限制为-1,0,1。目的参见DQN论文总结。

RewardWrapper是Wrapper的延伸，它添加了一个reward方法，我们可以在其中自定义我们希望对step所返回的reward所进行的处理。

```python
class ClipRewardEnv(gym.RewardWrapper):
    def __init__(self, env):
        gym.RewardWrapper.__init__(self, env)

    def reward(self, reward):
        """Bin reward to {+1, 0, -1} by its sign."""
        return np.sign(reward)
```

##### FrameStack

将4帧画面合并作为一个obs返回。这里用的是类似sliding window的更新方式，即buffer大小为4，每次往里面添加一帧画面并删除最旧的画面，然后同时返回buffer中的4帧画面。

```python
class FrameStack(gym.Wrapper):
    def __init__(self, env, k):
        """Stack k last frames.

        Returns lazy array, which is much more memory efficient.

        See Also
        --------
        baselines.common.atari_wrappers.LazyFrames
        """
        gym.Wrapper.__init__(self, env)
        self.k = k
        self.frames = deque([], maxlen=k)
        shp = env.observation_space.shape
        self.observation_space = spaces.Box(low=0, high=255, shape=(shp[0], shp[1], shp[2] * k), dtype=np.uint8)

    def reset(self):
        ob = self.env.reset()
        for _ in range(self.k):
            self.frames.append(ob)
        return self._get_ob()

    def step(self, action):
        ob, reward, done, info = self.env.step(action)
        self.frames.append(ob)
        return self._get_ob(), reward, done, info

    def _get_ob(self):
        assert len(self.frames) == self.k
        return LazyFrames(list(self.frames))
```

在atari_wrappers.py这个文件中，另外还有2个函数用于对这些wrappers进行封装。

```python
# 这个函数中添加了2个wrapper，可能openai认为，
# 对于atari来说,这两个wrapper是适用于所有baselines中的算法
def make_atari(env_id):
    env = gym.make(env_id)
    assert 'NoFrameskip' in env.spec.id
    env = NoopResetEnv(env, noop_max=30)
    env = MaxAndSkipEnv(env, skip=4)
    return env

# 这个函数名字中包含deepmind，可能openai认为，
# 这些操作由deepmind提出,但不是必须要做的
def wrap_deepmind(env, episode_life=True, clip_rewards=True, frame_stack=False, scale=False):
    """Configure environment for DeepMind-style Atari.
    """
    if episode_life:
        env = EpisodicLifeEnv(env)
    if 'FIRE' in env.unwrapped.get_action_meanings():
        env = FireResetEnv(env)
    env = WarpFrame(env)
    if scale:
        env = ScaledFloatFrame(env)
    if clip_rewards:
        env = ClipRewardEnv(env)
    if frame_stack:
        env = FrameStack(env, 4)
    return env
```



#### 3. 配置网络模型 Models

CNN

代码位于 `deepq/models.py` 文件夹下

#### 4. Train

基本根据DQN伪代码实现:  [伪代码](### 伪代码)

##### 代码实现:

训练循环基本上就是按照 DQN 的伪代码来写的，主要可以分为：

1. 初始化 replay buffer
2. 初始化网络参数
3. 采样
4. 训练 target Q 及周期性同步 behaviour Q

这里还多了日志和模型保存的部分。

代码位于 `deepq/simple.py` 文件夹下:

```python
def learn(env,
          q_func,
          lr=5e-4,
          max_timesteps=100000,
          buffer_size=50000,
          exploration_fraction=0.1,
          exploration_final_eps=0.02,
          train_freq=1,
          batch_size=32,
          print_freq=100,
          checkpoint_freq=10000,
          learning_starts=1000,
          gamma=1.0,
          target_network_update_freq=500,
          prioritized_replay=False,
          prioritized_replay_alpha=0.6,
          prioritized_replay_beta0=0.4,
          prioritized_replay_beta_iters=None,
          prioritized_replay_eps=1e-6,
          param_noise=False,
          callback=None):
    """Train a deepq model.

    Parameters
    -------
    env: gym.Env
        environment to train on
    q_func: (tf.Variable, int, str, bool) -> tf.Variable
        the model that takes the following inputs:
            observation_in: object
                the output of observation placeholder
            num_actions: int
                number of actions
            scope: str
            reuse: bool
                should be passed to outer variable scope
        and returns a tensor of shape (batch_size, num_actions) with values of every action.
    lr: float
        learning rate for adam optimizer
    max_timesteps: int
        number of env steps to optimizer for
    buffer_size: int
        size of the replay buffer
    exploration_fraction: float
        fraction of entire training period over which the exploration rate is annealed
    exploration_final_eps: float
        final value of random action probability
    train_freq: int
        update the model every `train_freq` steps.
        set to None to disable printing
    batch_size: int
        size of a batched sampled from replay buffer for training
    print_freq: int
        how often to print out training progress
        set to None to disable printing
    checkpoint_freq: int
        how often to save the model. This is so that the best version is restored
        at the end of the training. If you do not wish to restore the best version at
        the end of the training set this variable to None.
    learning_starts: int
        how many steps of the model to collect transitions for before learning starts
    gamma: float
        discount factor
    target_network_update_freq: int
        update the target network every `target_network_update_freq` steps.
    prioritized_replay: True
        if True prioritized replay buffer will be used.
    prioritized_replay_alpha: float
        alpha parameter for prioritized replay buffer
    prioritized_replay_beta0: float
        initial value of beta for prioritized replay buffer
    prioritized_replay_beta_iters: int
        number of iterations over which beta will be annealed from initial value
        to 1.0. If set to None equals to max_timesteps.
    prioritized_replay_eps: float
        epsilon to add to the TD errors when updating priorities.
    callback: (locals, globals) -> None
        function called at every steps with state of the algorithm.
        If callback returns true training stops.

    Returns
    -------
    act: ActWrapper
        Wrapper over act function. Adds ability to save it and load it.
        See header of baselines/deepq/categorical.py for details on the act function.
    """
    # Create all the functions necessary to train the model
    
    # 这两句话似乎相当于是with tf.Session（） as sess:
    sess = tf.Session()
    sess.__enter__()

    # capture the shape outside the closure so that the env object is not serialized
    # by cloudpickle when serializing make_obs_ph
    observation_space_shape = env.observation_space.shape
    def make_obs_ph(name):
        return BatchInput(observation_space_shape, 
                          =name)
	
	# 获得 act, train, update_target 3个主要的计算图封装函数
    # 其中, act即随机选择一个action的函数
    # 		train即 Optimize TD_error, 最小化td_error的huber_loss
    #		update_target为同步更新behaviour q和target q参数
    act, train, update_target, debug = deepq.build_train(
        make_obs_ph=make_obs_ph,
        q_func=q_func,
        num_actions=env.action_space.n,
        optimizer=tf.train.AdamOptimizer(learning_rate=lr),
        gamma=gamma,
        grad_norm_clipping=10,
        param_noise=param_noise
    )

    act_params = {
        'make_obs_ph': make_obs_ph,
        'q_func': q_func,
        'num_actions': env.action_space.n,
    }
	
	# 为 act 添加一个可以保存/载入 act 模型的 wrapper
    act = ActWrapper(act, act_params)
	
	### 1. 初始化 replay buffer
    # Create the replay buffer
    if prioritized_replay:
        replay_buffer = PrioritizedReplayBuffer(buffer_size, alpha=prioritized_replay_alpha)
        if prioritized_replay_beta_iters is None:
            prioritized_replay_beta_iters = max_timesteps
        beta_schedule = LinearSchedule(prioritized_replay_beta_iters,
                                       initial_p=prioritized_replay_beta0,
                                       final_p=1.0)
    else:
        replay_buffer = ReplayBuffer(buffer_size)
        beta_schedule = None
    # Create the schedule for exploration starting from 1.
    exploration = LinearSchedule(schedule_timesteps=int(exploration_fraction * max_timesteps),
                                 initial_p=1.0,
                                 final_p=exploration_final_eps)
	
	### 2. 初始化网络参数
    # Initialize the parameters and copy them to the target network.
    # 利用随机权值 \theta 来初始化动作-行为值函数Q
    U.initialize()
    # 令\theta^-=\theta 初始化用来 同步behaviour q和target q参数
    # update_target(), 是由deepq.build_train()返回的第三个计算图函数
    update_target()

    episode_rewards = [0.0]
    saved_mean_reward = None
    obs = env.reset()
    reset = True
    # 模型文件保存在/tmp， td是临时文件夹
    with tempfile.TemporaryDirectory() as td:
        model_saved = False
        model_file = os.path.join(td, "model")
        for t in range(max_timesteps):
            if callback is not None:
                if callback(locals(), globals()):
                    break
            # Take action and update exploration to the newest value
            kwargs = {}
			
			# 计算 epsilon 的值
            if not param_noise:
                update_eps = exploration.value(t)
                update_param_noise_threshold = 0.
            else:
                update_eps = 0.
                # Compute the threshold such that the KL divergence between perturbed and non-perturbed
                # policy is comparable to eps-greedy exploration with eps = exploration.value(t).
                # See Appendix C.1 in Parameter Space Noise for Exploration, Plappert et al., 2017
                # for detailed explanation.
                update_param_noise_threshold = -np.log(1. - exploration.value(t) + exploration.value(t) / float(env.action_space.n))
                kwargs['reset'] = reset
                kwargs['update_param_noise_threshold'] = update_param_noise_threshold
                kwargs['update_param_noise_scale'] = True
				
            ### 3. 采样
            # 随机选择一个action
            action = act(np.array(obs)[None], update_eps=update_eps, **kwargs)[0]
            env_action = action
            reset = False
			# 在环境中执行 action, 获得obs和reword
            new_obs, rew, done, _ = env.step(env_action)
            # Store transition in the replay buffer. 在replay buffer中存放新的转换<s,a,r,s'>.
            replay_buffer.add(obs, action, rew, new_obs, float(done))
            obs = new_obs
            
            # 这里的reward应该是clip过了的
            episode_rewards[-1] += rew
            if done:
                obs = env.reset()
                episode_rewards.append(0.0)
                reset = True
            
            ### 4. 训练 target Q
			# 每隔train_freq个step才会训练一次 target q, train_freq一般为4.每采样4次训练一次target 网络
            if t > learning_starts and t % train_freq == 0:
                # Minimize the error in Bellman's equation on a batch sampled from replay buffer.
				# 从 replay buffer中采样: 从回放记忆D中均匀随机采样一个转换样本数据
                if prioritized_replay:
                    experience = replay_buffer.sample(batch_size, beta=beta_schedule.value(t))
                    (obses_t, actions, rewards, obses_tp1, dones, weights, batch_idxes) = experience
                else:
                    obses_t, actions, rewards, obses_tp1, dones = replay_buffer.sample(batch_size)
                    weights, batch_idxes = np.ones_like(rewards), None
					
				# 过图训练:Optimize TD_error, 最小化loss, loss为td_error的huber_loss
                # train()函数是由deepq.build_train()返回的第二个计算图函数
                td_errors = train(obses_t, actions, rewards, obses_tp1, dones, weights)
				
                if prioritized_replay:
                    new_priorities = np.abs(td_errors) + prioritized_replay_eps
                    replay_buffer.update_priorities(batch_idxes, new_priorities)
            
            # 每隔target_network_update_freq个step才会同步一次behvaiour q和target q
            if t > learning_starts and t % target_network_update_freq == 0:
                # Update target network periodically.
                # 同步更新操作, 是由deepq.build_train()返回的第三个计算图函数
                update_target()
            
            ### 5. 日志及模型保存
            mean_100ep_reward = round(np.mean(episode_rewards[-101:-1]), 1)
            num_episodes = len(episode_rewards)
            if done and print_freq is not None and len(episode_rewards) % print_freq == 0:
                logger.record_tabular("steps", t)
                logger.record_tabular("episodes", num_episodes)
                logger.record_tabular("mean 100 episode reward", mean_100ep_reward)
                logger.record_tabular("% time spent exploring", int(100 * exploration.value(t)))
                logger.dump_tabular()
            
            # 保存最好的模型(mean_100ep_reward最大)
            if (checkpoint_freq is not None and t > learning_starts and
                    num_episodes > 100 and t % checkpoint_freq == 0):
                if saved_mean_reward is None or mean_100ep_reward > saved_mean_reward:
                    if print_freq is not None:
                        logger.log("Saving model due to mean reward increase: {} -> {}".format(
                                   saved_mean_reward, mean_100ep_reward))
                    save_state(model_file)
                    act.save(os.path.join(logger.get_dir(), 'model'))
                    model_saved = True
                    saved_mean_reward = mean_100ep_reward
        
        # 训练结束时的模型不一定比之前保存下来的模型好
        # 所以这一步会重新读取之前保存下来的最好的模型
        # 然后通过act把这个最好的模型返回出去
        if model_saved:
            if print_freq is not None:
                logger.log("Restored model with mean reward: {}".format(saved_mean_reward))
            load_state(model_file)

    return act
```

###### build_train()

 `build_train` 函数将训练用计算图封装成了几个可执行函数，包括采样用的 bahaviour policy `act_f`，训练 target policy Q network 用的 `train`，以及用于同步 target policy Q network 与 behvaiour policy Q network 的辅助函数 `update_target` 。

代码位于 `deepq/build_graph.py` 文件夹下:

关于几个计算图的说明:

```python
"""Deep Q learning graph

The functions in this file can are used to create the following functions:

======= act ========

    Function to chose an action given an observation

    Parameters
    ----------
    observation: object
        Observation that can be feed into the output of make_obs_ph
    stochastic: bool
        if set to False all the actions are always deterministic (default False)
    update_eps_ph: float
        update epsilon a new value, if negative not update happens
        (default: no update)

    Returns
    -------
    Tensor of dtype tf.int64 and shape (BATCH_SIZE,) with an action to be performed for
    every element of the batch.


======= act (in case of parameter noise) ========
    Function to chose an action given an observation
    Parameters
    ----------
    observation: object
        Observation that can be feed into the output of make_obs_ph
    stochastic: bool
        if set to False all the actions are always deterministic (default False)
    update_eps_ph: float
        update epsilon a new value, if negative not update happens
        (default: no update)
    reset_ph: bool
        reset the perturbed policy by sampling a new perturbation
    update_param_noise_threshold_ph: float
        the desired threshold for the difference between non-perturbed and perturbed policy
    update_param_noise_scale_ph: bool
        whether or not to update the scale of the noise for the next time it is re-perturbed

    Returns
    -------
    Tensor of dtype tf.int64 and shape (BATCH_SIZE,) with an action to be performed for
    every element of the batch.


======= train =======

    Function that takes a transition (s,a,r,s') and optimizes Bellman equation's error:

        td_error = Q(s,a) - (r + gamma * max_a' Q(s', a'))
        loss = huber_loss[td_error]

    Parameters
    ----------
    obs_t: object
        a batch of observations
    action: np.array
        actions that were selected upon seeing obs_t.
        dtype must be int32 and shape must be (batch_size,)
    reward: np.array
        immediate reward attained after executing those actions
        dtype must be float32 and shape must be (batch_size,)
    obs_tp1: object
        observations that followed obs_t
    done: np.array
        1 if obs_t was the last observation in the episode and 0 otherwise
        obs_tp1 gets ignored, but must be of the valid shape.
        dtype must be float32 and shape must be (batch_size,)
    weight: np.array
        imporance weights for every element of the batch (gradient is multiplied
        by the importance weight) dtype must be float32 and shape must be (batch_size,)

    Returns
    -------
    td_error: np.array
        a list of differences between Q(s,a) and the target in Bellman's equation.
        dtype is float32 and shape is (batch_size,)

======= update_target ========

    copy the parameters from optimized Q function to the target Q function.
    In Q learning we actually optimize the following error:

        Q(s,a) - (r + gamma * max_a' Q'(s', a'))

    Where Q' is lagging behind Q to stablize the learning. For example for Atari

    Q' is set to Q once every 10000 updates training steps.

"""
```

详细代码:

```python
def build_train(make_obs_ph, q_func, num_actions, optimizer, grad_norm_clipping=None, gamma=1.0, double_q=True, scope="deepq", reuse=None, param_noise=False, param_noise_filter_func=None):
    """Creates the train function:

    Parameters
    ----------
    make_obs_ph: str -> tf.placeholder or TfInput
        a function that takes a name and creates a placeholder of input with that name
    q_func: (tf.Variable, int, str, bool) -> tf.Variable
        the model that takes the following inputs:
            observation_in: object
                the output of observation placeholder
            num_actions: int
                number of actions
            scope: str
            reuse: bool
                should be passed to outer variable scope
        and returns a tensor of shape (batch_size, num_actions) with values of every action.
    num_actions: int
        number of actions
    reuse: bool
        whether or not to reuse the graph variables
    optimizer: tf.train.Optimizer
        optimizer to use for the Q-learning objective.
    grad_norm_clipping: float or None
        clip gradient norms to this value. If None no clipping is performed.
    gamma: float
        discount rate.
    double_q: bool
        if true will use Double Q Learning (https://arxiv.org/abs/1509.06461).
        In general it is a good idea to keep it enabled.
    scope: str or VariableScope
        optional scope for variable_scope.
    reuse: bool or None
        whether or not the variables should be reused. To be able to reuse the scope must be given.
    param_noise: bool
        whether or not to use parameter space noise (https://arxiv.org/abs/1706.01905)
    param_noise_filter_func: tf.Variable -> bool
        function that decides whether or not a variable should be perturbed. Only applicable
        if param_noise is True. If set to None, default_param_noise_filter is used by default.

    Returns
    -------
    act: (tf.Variable, bool, float) -> tf.Variable
        function to select and action given observation.
`       See the top of the file for details.
    train: (object, np.array, np.array, object, np.array, np.array) -> np.array
        optimize the error in Bellman's equation.
`       See the top of the file for details.
    update_target: () -> ()
        copy the parameters from optimized Q function to the target Q function.
`       See the top of the file for details.
    debug: {str: function}
        a bunch of functions to print debug data like q_values.
    """
	
	# 获得一个 act 封装函数: act_f, 由build_act()函数实现封装.
    if param_noise:
        act_f = build_act_with_param_noise(make_obs_ph, q_func, num_actions, scope=scope, reuse=reuse,
            param_noise_filter_func=param_noise_filter_func)
    else:
        act_f = build_act(make_obs_ph, q_func, num_actions, scope=scope, reuse=reuse)
	# train 封装函数的实现:
    with tf.variable_scope(scope, reuse=reuse):
        # set up placeholders
        # obs_t_input: t时刻的obs
        obs_t_input = make_obs_ph("obs_t")
        act_t_ph = tf.placeholder(tf.int32, [None], name="action")
        rew_t_ph = tf.placeholder(tf.float32, [None], name="reward")
        # obs_tp1_input: t+1 时刻的obs
        obs_tp1_input = make_obs_ph("obs_tp1")
        done_mask_ph = tf.placeholder(tf.float32, [None], name="done")
        importance_weights_ph = tf.placeholder(tf.float32, [None], name="weight")

        # q network evaluation
		# 这个是 behaviour policy 的Q
        # ！！！这里q_t的参数和上面act_f中的q_func是共享参数的
        # Q(st, a)
        # q_func()就是由tensorflow实现的CNN models.即神经网络
        q_t = q_func(obs_t_input.get(), num_actions, scope="q_func", reuse=True)  # reuse parameters from act
        q_func_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=tf.get_variable_scope().name + "/q_func")

        # target q network evalution
        # 注意这里q_tp1输入的obs与上面q_t不同，应该是t+1时刻的，上面的是t时刻的
        # Q^(st+1, a)
        q_tp1 = q_func(obs_tp1_input.get(), num_actions, scope="target_q_func")
        target_q_func_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=tf.get_variable_scope().name + "/target_q_func")
		
		# 根据采样的样本，计算q value
        # q scores for actions which we know were selected in the given state.
        # Q(st, a=selected_a)
        q_t_selected = tf.reduce_sum(q_t * tf.one_hot(act_t_ph, num_actions), 1)

        # compute estimate of best possible value starting from state at t + 1
        # Double DQN
        if double_q:
          #用behavior Policy的 网络参数theta, 来选择action
            q_tp1_using_online_net = q_func(obs_tp1_input.get(), num_actions, scope="q_func", reuse=True)
            # argmaxQ(s,a';theta)
            q_tp1_best_using_online_net = tf.argmax(q_tp1_using_online_net, 1)
            # tf.one_hot() 将argmaxQ(s,a';theta)获得的值转为概率分布, 用于选择action
            # tf.reduce_sum() 计算和, q_tp1是target Q 的神经网络,...最后得到的是用target Q的网络参数计算的Q值.
            q_tp1_best = tf.reduce_sum(q_tp1 * tf.one_hot(q_tp1_best_using_online_net, num_actions), 1)
        else:
            # maxQ^(st+1, a)
            q_tp1_best = tf.reduce_max(q_tp1, 1)
        # 当episode结束时，maxQ^(st+1, a)不存在，所以done_mask_ph为1, q_tp1_best_masked设为0, 只要episode没有结束, done_mask_ph值均为0.
        q_tp1_best_masked = (1.0 - done_mask_ph) * q_tp1_best

        # compute RHS of bellman equation
        # td target: r + gamma * maxQ^(st+1,a)
        q_t_selected_target = rew_t_ph + gamma * q_tp1_best_masked

        # compute the error (potentially clipped)
		# 计算td error
        # 这个减法应该反了，但是因为下面的loss不受符号影响，所以也没关系
		# 这里的stop_gradient的作用不清楚
        # 不对q_t_selected_target求导是因为不需要对Q^进行更新吗？
        td_error = q_t_selected - tf.stop_gradient(q_t_selected_target)
		
		# 神经网络 loss
		errors = U.huber_loss(td_error)
        weighted_error = tf.reduce_mean(importance_weights_ph * errors)

        # compute optimization op (potentially with gradient clipping)
        # 不确定stop_gradient(q_t_selected_target)和var_list=q_func_vars是不是等效的
        if grad_norm_clipping is not None:
            gradients = optimizer.compute_gradients(weighted_error, var_list=q_func_vars)
            for i, (grad, var) in enumerate(gradients):
                if grad is not None:
                    gradients[i] = (tf.clip_by_norm(grad, grad_norm_clipping), var)
            optimize_expr = optimizer.apply_gradients(gradients)
        else:
            optimize_expr = optimizer.minimize(weighted_error, var_list=q_func_vars)
		
        # update_target 封装函数的实现: 
        # update_target_fn will be called periodically to copy Q network to target Q network
		# update_target_expr 用于同步 target policy Q net 与 behaviour policy Q net
        update_target_expr = []
        for var, var_target in zip(sorted(q_func_vars, key=lambda v: v.name),
                                   sorted(target_q_func_vars, key=lambda v: v.name)):
            update_target_expr.append(var_target.assign(var))
        # update_target_expr是一系列assign操作的集合
        update_target_expr = tf.group(*update_target_expr)

        # Create callable functions:将train计算图通过函数function(), 转换为一个可执行的函数.
		# 输入: st, at, st+1, rt, done_mask_ph, importance_weights_ph
		# 输出：td_error
        train = U.function(
            inputs=[
                obs_t_input,
                act_t_ph,
                rew_t_ph,
                obs_tp1_input,
                done_mask_ph,
                importance_weights_ph
            ],
            outputs=td_error,
            updates=[optimize_expr]     # 这部分负责BP，需要执行但不需要输出
        )
        
		# 用于同步 target policy Q net 与 behaviour policy Q net
        # 下面这个的调用频率和上面的train不一样，所以必须分成2个函数
        update_target = U.function([], [], updates=[update_target_expr])
        
        # debug用
        q_values = U.function([obs_t_input], q_t)

        return act_f, train, update_target, {'q_values': q_values}
```

可以看到其中, `act_f` 又是通过函数 `build_act` 函数实现的:

###### build_act()

 `build_act` 实现了 agent 的计算图，即给定输入图片，输出 action。我们在训练和测试时都会用到它。而`build_train` 实现了训练用的计算图，仅在训练时使用。

 `build_act`这个函数将 agent，利用上面提到的`Function`，封装进了一个名为 `act` 的函数中。`act` 主要包括了一个计算 Q function 的神经网络，以及将 Q function 转换为 action 的 $\epsilon$-greedy 算法。也就是说，只要我们将游戏画面输入 `act`，我们就可以得到 action。

代码位于 `deepq/build_graph.py` 文件夹下:

```python
def build_act(make_obs_ph, q_func, num_actions, scope="deepq", reuse=None):
    """Creates the act function:

    Parameters
    ----------
    make_obs_ph: str -> tf.placeholder or TfInput
        a function that take a name and creates a placeholder of input with that name
    q_func: (tf.Variable, int, str, bool) -> tf.Variable
        the model that takes the following inputs:
            observation_in: object
                the output of observation placeholder
            num_actions: int
                number of actions
            scope: str
            reuse: bool
                should be passed to outer variable scope
        and returns a tensor of shape (batch_size, num_actions) with values of every action.
    num_actions: int
        number of actions.
    scope: str or VariableScope
        optional scope for variable_scope.
    reuse: bool or None
        whether or not the variables should be reused. To be able to reuse the scope must be given.

    Returns
    -------
    act: (tf.Variable, bool, float) -> tf.Variable
        function to select and action given observation.
`       See the top of the file for details.
    """
    with tf.variable_scope(scope, reuse=reuse):
		# agent 的输入
        # obs输入，是一个自定义的BatchInput，需要通过BatchInput.get()获得其内部的placeholder
        observations_ph = make_obs_ph("observation")
		
		# 下面这2个是输入参数
        # 用于选择e-greedy或者greedy
        stochastic_ph = tf.placeholder(tf.bool, (), name="stochastic")
        # 用于更新epsilon的值
        update_eps_ph = tf.placeholder(tf.float32, (), name="update_eps")
        
        # 计算图内部储存 \epsilon 的变量, \epsilon : 随机选择action的概率
        # eps首先初始化为0, 在后续会更新eps
        eps = tf.get_variable("eps", (), initializer=tf.constant_initializer(0))
        
        # 表征q function的神经网络主体
        q_values = q_func(observations_ph.get(), num_actions, scope="q_func")
        # greedy action:  argmaxQ(s,a)
        deterministic_actions = tf.argmax(q_values, axis=1)

        batch_size = tf.shape(observations_ph.get())[0]
		
        # 随机action
        random_actions = tf.random_uniform(tf.stack([batch_size]), minval=0, maxval=num_actions, dtype=tf.int64)
        # condition定义:概率小于eps则为True
        chose_random = tf.random_uniform(tf.stack([batch_size]), minval=0, maxval=1, dtype=tf.float32) < eps
        # e-greedy action: 以概率eps选择一个随机动作random_actions, 如果此概率没有发生, 用贪婪策略选择当前值函数最大的那个动作, 即 argmaxQ(s,a):deterministic_actions.
        stochastic_actions = tf.where(chose_random, random_actions, deterministic_actions)
        # tf.where(condition, x, y,name=None), x、y必须有相同的形状, condition为bools型, 如果condition值为True 那么返回x的值，否则为y的值.  
        
        # 根据stochastic_ph选择输出的action是否随机
        output_actions = tf.cond(stochastic_ph, lambda: stochastic_actions, lambda: deterministic_actions)
        
        # epsilon(即eps, 概率)更新逻辑
        # assign()重新赋值, 
        update_eps_expr = eps.assign(tf.cond(update_eps_ph >= 0, lambda: update_eps_ph, lambda: eps))
        
        # 封装成可直接运行的函数
		# 函数输入：observations_ph, stochastic_ph, update_eps_ph
		# 函数输出：output_actions
		# 默认 epsilon=0，且不会更新
		# 默认使用e-greedy
        _act = U.function(inputs=[observations_ph, stochastic_ph, update_eps_ph],
                         outputs=output_actions,
                         givens={update_eps_ph: -1.0, stochastic_ph: True},
                         updates=[update_eps_expr])
        def act(ob, stochastic=True, update_eps=-1):
            return _act(ob, stochastic, update_eps)
        return act
```



#### 5. 结束: env.close()
